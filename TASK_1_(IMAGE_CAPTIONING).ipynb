{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O85iFRiNC6Kf"
      },
      "source": [
        "**IMAGE CAPTIONING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nN6y1fKC_b-"
      },
      "source": [
        "STEP 1: IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LlpUvwn-CzKo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0RLJrKrDJir"
      },
      "source": [
        "STEP 2: LOAD PRE-TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "01XETgoRDGBc"
      },
      "outputs": [],
      "source": [
        "# Using VGG16 for feature extraction\n",
        "base_model = VGG16(weights='imagenet')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcWut-tpDc1G"
      },
      "source": [
        "STEP 3: PREPROCESS IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TeFqCD1KDYWy"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "img_path = '/content/AI IMAGE.jpg'\n",
        "img = preprocess_image(img_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CseC1YmWDsc5"
      },
      "source": [
        "STEP 4: EXTRACT FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpaMizcBD1fe",
        "outputId": "63f1e03e-5898-4bf1-fc8a-6b8b8426dd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ],
      "source": [
        "features = model.predict(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho81XrIzD5gI"
      },
      "source": [
        "STEP 5: PREPARE TEXT DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ycQ7aZtJD2Ry"
      },
      "outputs": [],
      "source": [
        "# Example captions\n",
        "captions = [\"a man riding a horse\", \"a person on a horse in a field\"]\n",
        "\n",
        "# Tokenize the captions\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert captions to sequences\n",
        "sequences = tokenizer.texts_to_sequences(captions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEaJxhIzECaP"
      },
      "source": [
        "STEP 6: CREATE EMBEDDING LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZmdoNhdvD_IQ"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9x3Zga5EK6L"
      },
      "source": [
        "STEP 7: BUILD THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bi5nBhUUEHmh"
      },
      "outputs": [],
      "source": [
        "# Image feature input\n",
        "image_input = Input(shape=(4096,))\n",
        "image_model = Dropout(0.5)(image_input)\n",
        "image_model = Dense(256, activation='relu')(image_model)\n",
        "\n",
        "# Caption input\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_model = embedding_layer(caption_input)\n",
        "caption_model = Dropout(0.5)(caption_model)\n",
        "caption_model = LSTM(256)(caption_model)\n",
        "\n",
        "# Combine image and caption models\n",
        "decoder = add([image_model, caption_model])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8aOdefnsoJp"
      },
      "source": [
        "STEP 8: TRAIN THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAWCdZ_0syS_",
        "outputId": "4adf6426-bf31-4099-d275-0cbdd9105ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "\n",
        "# Load ResNet50 model pre-trained on ImageNet, exclude the top layer\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Extract image features\n",
        "def extract_image_features(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    features = resnet.predict(img)\n",
        "    return features\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/AI IMAGE.jpg'\n",
        "image_features = extract_image_features(image_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mePMYngfFt4a"
      },
      "source": [
        "STEP 9: GENERATE CAPTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HuniqnbFdkz",
        "outputId": "21f3fa2c-1024-42ff-8f9f-f4489974eb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " caption_input (InputLayer)  [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " image_input (InputLayer)    [(None, 2048)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 256)            2304      ['caption_input[0][0]']       \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 512)                  1049088   ['image_input[0][0]']         \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 512)                  1574912   ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 512)                  0         ['dense_3[0][0]',             \n",
            "                                                                     'lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 512)                  262656    ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 9)                    4617      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2893577 (11.04 MB)\n",
            "Trainable params: 2893577 (11.04 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "\n",
        "# Example word index mapping (replace with actual mappings)\n",
        "word_index = {'startseq': 1, 'endseq': 2, 'a': 3, 'cat': 4, 'sits': 5, 'on': 6, 'the': 7, 'mat': 8}\n",
        "index_word = {index: word for word, index in word_index.items()}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "max_caption_length = 20  # Example value\n",
        "\n",
        "# Model architecture\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "image_input = Input(shape=(2048,), name=\"image_input\")\n",
        "caption_input = Input(shape=(None,), name=\"caption_input\")\n",
        "\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(caption_input)\n",
        "lstm = LSTM(lstm_units)(embedding)\n",
        "\n",
        "# Transform image features to the same shape as the LSTM output\n",
        "image_features_transformed = Dense(lstm_units, activation='relu')(image_input)\n",
        "\n",
        "decoder1 = add([image_features_transformed, lstm])\n",
        "decoder2 = Dense(lstm_units, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Assuming the model has been trained and weights loaded\n",
        "# model.load_weights('path_to_weights.h5')\n",
        "\n",
        "def generate_caption(image_feature):\n",
        "    caption = [word_index['startseq']]\n",
        "    for _ in range(max_caption_length):\n",
        "        sequence = pad_sequences([caption], maxlen=max_caption_length)\n",
        "        y_pred = model.predict([image_feature, sequence], verbose=0)\n",
        "        y_pred = np.argmax(y_pred)\n",
        "        word = index_word[y_pred]\n",
        "        caption.append(y_pred)\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return ' '.join([index_word[idx] for idx in caption if idx not in [word_index['startseq'], word_index['endseq']]])\n",
        "\n",
        "# Example usage with dummy image features\n",
        "features = np.random.rand(1, 2048)  # Example feature\n",
        "caption = generate_caption(features)\n",
        "print(caption)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv9vvv03Gle-"
      },
      "source": [
        "STEP 10: EVALUATE THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq6WqZdHFz20",
        "outputId": "ee89e492-ab0e-4255-e9ed-c222a6f5b70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "startseq a a a a a a a a\n"
          ]
        }
      ],
      "source": [
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word[yhat]\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# Example usage\n",
        "caption = generate_caption(model, tokenizer, features, max_length)\n",
        "print(caption)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3BK9zy_GukF"
      },
      "source": [
        "STEP 11: FINE-TUNE THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJVI2VnWIzJM",
        "outputId": "15ae03d9-d3d6-4141-8cea-d4767cad65d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'horse': 2, 'man': 3, 'riding': 4, 'person': 5, 'on': 6, 'in': 7, 'field': 8}\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.word_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL4yzMuBJAmt",
        "outputId": "0a1b7746-d4f2-4357-bdcb-7cd7a4cf37f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'startseq': 1, 'example': 2, 'endseq': 3, 'this': 4, 'is': 5, 'an': 6, 'sentence': 7, 'another': 8, 'here': 9}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Example training data including special tokens\n",
        "texts = [\n",
        "    'startseq this is an example sentence endseq',\n",
        "    'startseq another example here endseq'\n",
        "]\n",
        "\n",
        "# Fit tokenizer on your texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Add special tokens manually if needed\n",
        "start_token = 'startseq'\n",
        "end_token = 'endseq'\n",
        "\n",
        "# Ensure they are in the tokenizer\n",
        "if start_token not in tokenizer.word_index:\n",
        "    tokenizer.word_index[start_token] = len(tokenizer.word_index) + 1\n",
        "if end_token not in tokenizer.word_index:\n",
        "    tokenizer.word_index[end_token] = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXfGcEFgKBmN"
      },
      "source": [
        "STEP 12: DEPLOY THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('image_captioning_model.h5')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('image_captioning_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "wEGP7nrde_SH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76119a92-54af-4852-f0a2-f82ab935c708"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 13: CONTINUOUS IMPROVEMENT\n",
        "\n",
        "Collect feedback, retrain with new data, and iteratively improve the model based on performance."
      ],
      "metadata": {
        "id": "SmQQ0w9Psv56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 14: HANDLING ERRORS AND EDGE CASES"
      ],
      "metadata": {
        "id": "xfj8alGMf8TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Code that might fail\n",
        "    caption = generate_caption(features)\n",
        "except Exception as e:\n",
        "    print(f'Error generating caption: {e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8tZbFffw8d",
        "outputId": "20a8754e-ca8e-4757-9358-bdff928455c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating caption: generate_caption() missing 3 required positional arguments: 'tokenizer', 'photo', and 'max_length'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}